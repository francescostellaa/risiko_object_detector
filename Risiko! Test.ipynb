{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880f2960",
   "metadata": {},
   "source": [
    "# <b>Deep Learning:</b> Risiko! Army detector\n",
    "\n",
    "---\n",
    "A.A. 2022/23 (6 CFU) - Dr. Daniel Fusaro Ph.D. Student\n",
    "\n",
    "---\n",
    "\n",
    "# Yolov7 test\n",
    "\n",
    "\n",
    "With this Jupyter Notebook you can:\n",
    " - test the accuracy of a pre-trained Yolov7 model on your custom dataset\n",
    " - get the performance metrics (that you need to include in your project report)\n",
    " - visualize the results\n",
    "\n",
    "---\n",
    "\n",
    "Risiko! game official Wikipedia page -\n",
    "[https://en.wikipedia.org/wiki/RisiKo!](https://en.wikipedia.org/wiki/RisiKo!)\n",
    "\n",
    "---\n",
    "\n",
    "### To run this Jupyter Notebook you will need\n",
    "\n",
    "#### run the following commands:\n",
    "```\n",
    "pip3 install torch torchvision\n",
    "pip3 install seaborn\n",
    "git clone https://github.com/WongKinYiu/yolov7\n",
    " ```\n",
    "#### follow these steps:\n",
    " - the weights of the pretrained model (see the Google Drive folder of the project)\n",
    " - the dataset produced by you (using Risiko! Synthetic Dataset Creator.ipynb)\n",
    " - the dataset of real images that you find in the Google Drive folder of the project\n",
    " - modify the coco_risiko.yaml config file: it must contain, in the \"test\" value, the path to the test.txt file containing all the test images path\n",
    " - produce the aforementioned test.txt file, like the one you find in the Google Drive folder (aka test_example.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c15c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch                    # if you don't have it, just \"pip3 install torch torchvision\"\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "sys.path.append('yolov7')\n",
    "\n",
    "# probably, you also must do \"pip3 install seaborn\"\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "from utils.plots import plot_images, output_to_target, plot_study_txt\n",
    "from utils.torch_utils import select_device, time_synchronized, TracedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5fe15",
   "metadata": {},
   "source": [
    "## The \"test\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2292ff85",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def test(data,\n",
    "         weights=None,\n",
    "         batch_size=32,\n",
    "         imgsz=640,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,      # for NMS\n",
    "         save_json=False,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         verbose=False,\n",
    "         model=None,\n",
    "         dataloader=None,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         save_txt=False,     # for auto-labelling\n",
    "         save_hybrid=False,  # for hybrid auto-labelling\n",
    "         save_conf=False,    # save auto-label confidences\n",
    "         plots=True,\n",
    "         wandb_logger=None,\n",
    "         compute_loss=None,\n",
    "         half_precision=True,\n",
    "         trace=False,\n",
    "         is_coco=False,\n",
    "         v5_metric=False):\n",
    "\n",
    "    set_logging()\n",
    "    device = select_device(opt.device, batch_size=batch_size)\n",
    "\n",
    "    # Directories\n",
    "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
    "\n",
    "    if trace:\n",
    "        model = TracedModel(model, device, imgsz)\n",
    "\n",
    "    # Half\n",
    "    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    if isinstance(data, str):\n",
    "        is_coco = data.endswith('coco.yaml')\n",
    "        with open(data) as f:\n",
    "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Logging\n",
    "    log_imgs = 0\n",
    "    if wandb_logger and wandb_logger.wandb:\n",
    "        log_imgs = min(wandb_logger.log_imgs, 100)\n",
    "    # Dataloader\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "    dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n",
    "                                   prefix=colorstr(f'{task}: '))[0]\n",
    "\n",
    "    if v5_metric:\n",
    "        print(\"Testing with YOLOv5 AP metric...\")\n",
    "    \n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = time_synchronized()\n",
    "            out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "            t0 += time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if compute_loss:\n",
    "                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
    "\n",
    "            t = time_synchronized()\n",
    "            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
    "            t1 += time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path = Path(paths[si])\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
    "            \n",
    "            if opt.print_bbox:\n",
    "                for line in predn:\n",
    "                    x, y, w, h, conf, cl = line.detach().cpu().float()\n",
    "                    print(f\"{int(x):<6.0f} {int(y):<6.0f} {int(w):<6.0f} {int(h):<6.0f} {conf:<6.4} {int(cl):<2d}\")\n",
    "            \n",
    "            opt.labels.append([predn, paths[si]])\n",
    "            \n",
    "            # Append to text file\n",
    "            if save_txt:\n",
    "                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                for *xyxy, conf, cls in predn.tolist():\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            # W&B logging - Media Panel Plots\n",
    "            if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n",
    "                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
    "                    box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
    "                                 \"class_id\": int(cls),\n",
    "                                 \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
    "                                 \"scores\": {\"class_score\": conf},\n",
    "                                 \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
    "                    boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n",
    "                    wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
    "            wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "                box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append({'image_id': image_id,\n",
    "                                  'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
    "                                  'bbox': [round(x, 3) for x in b],\n",
    "                                  'score': round(p[4], 5)})\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "        # Plot images\n",
    "        if plots and batch_i < 3:\n",
    "            f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n",
    "            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
    "            f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n",
    "            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, v5_metric=v5_metric, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if (verbose or nc < 50) and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
    "    print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "    # Plots\n",
    "    if plots:\n",
    "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "        if wandb_logger and wandb_logger.wandb:\n",
    "            val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n",
    "            wandb_logger.log({\"Validation\": val_batches})\n",
    "    if wandb_images:\n",
    "        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and len(jdict):\n",
    "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
    "        anno_json = './coco/annotations/instances_val2017.json'  # annotations json\n",
    "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "        print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n",
    "        with open(pred_json, 'w') as f:\n",
    "            json.dump(jdict, f)\n",
    "\n",
    "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            anno = COCO(anno_json)  # init annotations api\n",
    "            pred = anno.loadRes(pred_json)  # init predictions api\n",
    "            eval = COCOeval(anno, pred, 'bbox')\n",
    "            if is_coco:\n",
    "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "            eval.evaluate()\n",
    "            eval.accumulate()\n",
    "            eval.summarize()\n",
    "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except Exception as e:\n",
    "            print(f'pycocotools unable to run: {e}')\n",
    "\n",
    "    # Return results\n",
    "    s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "    print(f\"Results saved to {save_dir}{s}\")\n",
    "    \n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc6471",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86f8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    data        = \"coco_risiko.yaml\"\n",
    "    weights     = \"pre_trained_weights/pre_trained_yolo_weights_v00.pt\" ##### PUT HERE the path to the weights of yolo (e.h. pre_trained_weights/pre_trained_yolo_weights_v00.pt)\n",
    "    \n",
    "    batch_size  = 1                           # size of each image batch\n",
    "    img_size    = 640                         # inference size (pixels)\n",
    "    conf_thres  = 0.2                         # object confidence threshold\n",
    "    iou_thres   = 0.65                        # IOU threshold for NMS\n",
    "    save_json   = False                       # save a cocoapi-compatible JSON results file\n",
    "    single_cls  = False\n",
    "    augment     = False\n",
    "    verbose     = False\n",
    "    save_txt    = False                        # save results to *.txt\n",
    "    save_hybrid = False                        # save label+prediction hybrid results to *.txt\n",
    "    save_conf   = False                        # save confidences in --save-txt labels\n",
    "    project     = \"runs/test\"                 \n",
    "    no_trace    = False                        # don`t trace model\n",
    "    v5_metric   = False                        # assume maximum recall as 1.0 in AP calculation\n",
    "    exist_ok    = False                        # existing project/name ok, do not increment\n",
    "    device      = \"cpu\"                         # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    task        = \"test\"                      # \n",
    "    name        = \"yolov7_risiko_test\"        # save to project/name\n",
    "    \n",
    "    print_bbox  = False\n",
    "    \n",
    "    labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9456b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR 🚀 v0.1-128-ga207844 torch 2.3.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 314 layers, 36541106 parameters, 6194944 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning 'synthetic_dataset/labels' images and labels... 1024 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 1024/1024 [00:00<00:00, 2116.91it/s]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: synthetic_dataset/labels.cache\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 1024/1024 [08:48<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all        1024       53199        0.82       0.438       0.436       0.349\n",
      "           blue_army        1024        7143       0.862       0.488       0.486       0.395\n",
      "            red_army        1024        7080        0.84       0.461       0.459       0.371\n",
      "         yellow_army        1024        7103         0.9         0.4       0.402        0.33\n",
      "         purple_army        1024        7028       0.804       0.477       0.471       0.381\n",
      "          black_army        1024        7171       0.787        0.47       0.464        0.38\n",
      "          green_army        1024        7091       0.875       0.488       0.487       0.407\n",
      "           blue_flag        1024        1816       0.829       0.405       0.405       0.321\n",
      "            red_flag        1024        1784       0.764       0.419       0.412       0.322\n",
      "         yellow_flag        1024        1748       0.813       0.382        0.38        0.29\n",
      "         purple_flag        1024        1700       0.767       0.429       0.424       0.326\n",
      "          black_flag        1024        1783       0.778         0.4       0.398        0.32\n",
      "          green_flag        1024        1752       0.817       0.439       0.438       0.342\n",
      "Speed: 504.0/0.4/504.4 ms inference/NMS/total per 640x640 image at batch-size 1\n",
      "Results saved to runs/test/yolov7_risiko_test10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.8198018731666089,\n",
       "  0.43813778096140293,\n",
       "  0.43567546890320674,\n",
       "  0.3488691372665564,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " array([    0.39495,     0.37133,     0.33035,     0.38131,     0.37987,     0.40719,      0.3214,     0.32189,     0.29012,     0.32629,     0.31978,     0.34194]),\n",
       " (503.9699384942651, 0.4082473460584879, 504.37818584032357, 640, 640, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Opt()\n",
    "\n",
    "opt.data = check_file(opt.data)\n",
    "\n",
    "test(opt.data,\n",
    "     opt.weights,\n",
    "     opt.batch_size,\n",
    "     opt.img_size,\n",
    "     opt.conf_thres,\n",
    "     opt.iou_thres,\n",
    "     opt.save_json,\n",
    "     opt.single_cls,\n",
    "     opt.augment,\n",
    "     opt.verbose,\n",
    "     save_txt = opt.save_txt | opt.save_hybrid,\n",
    "     save_hybrid = opt.save_hybrid,\n",
    "     save_conf = opt.save_conf,\n",
    "     trace = not opt.no_trace,\n",
    "     v5_metric = opt.v5_metric\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c48c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_dataset/images/000000.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/8j5njktx433_09bwgn76rs840000gn/T/ipykernel_54821/2276961729.py:2: MatplotlibDeprecationWarning: Auto-close()ing of figures upon backend switching is deprecated since 3.8 and will be removed two minor releases later.  To suppress this warning, explicitly call plt.close('all') first.\n",
      "  matplotlib.use('qtagg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_dataset/images/000675.jpg\n",
      "synthetic_dataset/images/000676.jpg\n",
      "synthetic_dataset/images/000677.jpg\n",
      "synthetic_dataset/images/000678.jpg\n",
      "synthetic_dataset/images/000679.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(img, (\u001b[38;5;28mint\u001b[39m(x), \u001b[38;5;28mint\u001b[39m(y)), (\u001b[38;5;28mint\u001b[39m(w), \u001b[38;5;28mint\u001b[39m(h)), (\u001b[38;5;241m36\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m cl \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/pyplot.py:527\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/backend_bases.py:3457\u001b[0m, in \u001b[0;36m_Backend.show\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3455\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m ipython_pylab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive()\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m-> 3457\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/backends/backend_qt.py:593\u001b[0m, in \u001b[0;36mFigureManagerQT.start_main_loop\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    591\u001b[0m qapp \u001b[38;5;241m=\u001b[39m QtWidgets\u001b[38;5;241m.\u001b[39mQApplication\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qapp:\n\u001b[0;32m--> 593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_maybe_allow_interrupt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqapp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqt_compat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqapp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/backends/qt_compat.py:230\u001b[0m, in \u001b[0;36m_maybe_allow_interrupt\u001b[0;34m(qapp)\u001b[0m\n\u001b[1;32m    228\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, old_sigint_handler)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mold_sigint_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhandler_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import PyQt5\n",
    "matplotlib.use('qtagg')\n",
    "\n",
    "for labels, image_path in opt.labels:\n",
    "    img = cv2.imread(image_path)\n",
    "    print(image_path)\n",
    "    for line in labels:\n",
    "        x, y, w, h, conf, cl = line.detach().cpu().float()\n",
    "        cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (36,255,12, 1) if cl < 6 else (255,12,10, 1), 2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
